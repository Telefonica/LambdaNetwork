/home/b.btv/anaconda3/envs/tid_lambda/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: "sox" backend is being deprecated. The default backend will be changed to "sox_io" backend in 0.8.0 and "sox" backend will be removed in 0.9.0. Please migrate to "sox_io" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.
  warnings.warn(
Model is SupervisedModel
84843
9981
11005
Computing the occurences of each class in the dataset (10)...
(0.00%) Evaluated 0/84843 samples
(11.79%) Evaluated 10000/84843 samples
(23.57%) Evaluated 20000/84843 samples
(35.36%) Evaluated 30000/84843 samples
(47.15%) Evaluated 40000/84843 samples
(58.93%) Evaluated 50000/84843 samples
(70.72%) Evaluated 60000/84843 samples
(82.51%) Evaluated 70000/84843 samples
(94.29%) Evaluated 80000/84843 samples
Class weights: [27.07179324 27.31584031 27.93645044 27.10638978 28.56666667 27.49287103
 28.10301424 27.27193828  1.56901653 28.77985075 26.28345725]
84843
84843
-----------------------
/home/b.btv/anaconda3/envs/tid_lambda/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/SpectralOps.cpp:653.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore
/home/b.btv/anaconda3/envs/tid_lambda/lib/python3.8/site-packages/torch/functional.py:515: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/aten/src/ATen/native/SpectralOps.cpp:590.)
  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore
torch.Size([1, 40, 101])
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv1d-1             [256, 44, 101]           5,280
            Conv1d-2             [256, 44, 101]           5,808
       BatchNorm1d-3             [256, 44, 101]               0
            Conv1d-4             [256, 64, 101]           2,816
       BatchNorm1d-5             [256, 64, 101]             128
            Conv1d-6             [256, 16, 101]             704
           Softmax-7          [256, 16, 1, 101]               0
            Conv1d-8             [256, 11, 101]             484
       BatchNorm1d-9             [256, 11, 101]              22
    LambdaLayer1D-10             [256, 44, 101]               0
      BatchNorm1d-11             [256, 44, 101]               0
           Conv1d-12             [256, 44, 101]           5,808
      BatchNorm1d-13             [256, 44, 101]               0
           Conv1d-14             [256, 64, 101]           2,816
      BatchNorm1d-15             [256, 64, 101]             128
           Conv1d-16             [256, 16, 101]             704
          Softmax-17          [256, 16, 1, 101]               0
           Conv1d-18             [256, 11, 101]             484
      BatchNorm1d-19             [256, 11, 101]              22
    LambdaLayer1D-20             [256, 44, 101]               0
      BatchNorm1d-21             [256, 44, 101]               0
           Conv1d-22             [256, 44, 101]           5,808
      BatchNorm1d-23             [256, 44, 101]               0
           Conv1d-24             [256, 64, 101]           2,816
      BatchNorm1d-25             [256, 64, 101]             128
           Conv1d-26             [256, 16, 101]             704
          Softmax-27          [256, 16, 1, 101]               0
           Conv1d-28             [256, 11, 101]             484
      BatchNorm1d-29             [256, 11, 101]              22
    LambdaLayer1D-30             [256, 44, 101]               0
      BatchNorm1d-31             [256, 44, 101]               0
           Conv1d-32             [256, 44, 101]           5,808
      BatchNorm1d-33             [256, 44, 101]               0
           Conv1d-34             [256, 64, 101]           2,816
      BatchNorm1d-35             [256, 64, 101]             128
           Conv1d-36             [256, 16, 101]             704
          Softmax-37          [256, 16, 1, 101]               0
           Conv1d-38             [256, 11, 101]             484
      BatchNorm1d-39             [256, 11, 101]              22
    LambdaLayer1D-40             [256, 44, 101]               0
      BatchNorm1d-41             [256, 44, 101]               0
           Conv1d-42             [256, 44, 101]           5,808
      BatchNorm1d-43             [256, 44, 101]               0
           Conv1d-44             [256, 64, 101]           2,816
      BatchNorm1d-45             [256, 64, 101]             128
           Conv1d-46             [256, 16, 101]             704
          Softmax-47          [256, 16, 1, 101]               0
           Conv1d-48             [256, 11, 101]             484
      BatchNorm1d-49             [256, 11, 101]              22
    LambdaLayer1D-50             [256, 44, 101]               0
      BatchNorm1d-51             [256, 44, 101]               0
           Conv1d-52             [256, 44, 101]           5,808
      BatchNorm1d-53             [256, 44, 101]               0
           Conv1d-54             [256, 64, 101]           2,816
      BatchNorm1d-55             [256, 64, 101]             128
           Conv1d-56             [256, 16, 101]             704
          Softmax-57          [256, 16, 1, 101]               0
           Conv1d-58             [256, 11, 101]             484
      BatchNorm1d-59             [256, 11, 101]              22
    LambdaLayer1D-60             [256, 44, 101]               0
      BatchNorm1d-61             [256, 44, 101]               0
           Conv1d-62             [256, 44, 101]           5,808
      BatchNorm1d-63             [256, 44, 101]               0
           Conv1d-64             [256, 64, 101]           2,816
      BatchNorm1d-65             [256, 64, 101]             128
           Conv1d-66             [256, 16, 101]             704
          Softmax-67          [256, 16, 1, 101]               0
           Conv1d-68             [256, 11, 101]             484
      BatchNorm1d-69             [256, 11, 101]              22
    LambdaLayer1D-70             [256, 44, 101]               0
      BatchNorm1d-71             [256, 44, 101]               0
LambdaResNet15_1d-72                  [256, 44]               0
           Linear-73                  [256, 44]           1,980
             ReLU-74                  [256, 44]               0
           Linear-75                  [256, 11]             495
  SupervisedModel-76                  [256, 11]               0
================================================================
Total params: 77,489
Trainable params: 77,489
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 3.95
Forward/backward pass size (MB): 503.33
Params size (MB): 0.30
Estimated Total Size (MB): 507.57
----------------------------------------------------------------

(tensor(77489), tensor(77489))
Train transforms: Sequential(
  (0): VolTransform(gain_type=db, min=-5, max=5, p=0.5)
  (1): CropTransform(miliseconds=100, p=0.5)
  (2): TimeStretchTransform(min_rate=0.75, max_rate=1.25, p=0.3)
  (3): PitchShiftTransform(min_semit=-4, max_semit=4, p=0.3)
  (4): ShiftTransform(min_fract=-0.2, max_fract=0.2, rollover=False, p=0.3)
  (5): ClippingDistortionTransform(min_th=20, max_th=40, p=0.2)
  (6): AddBackgroundNoiseSNR(SNR_range(dB)=(0, 15)
  (7): LengthTransform(length=16000)
)
Validation transforms: Sequential(
  (0): LengthTransform(length=16000)
)
Dataset contains 84843/9981 train/val samples
Criterion is CrossEntropyLoss
SGD (
Parameter Group 0
    dampening: 0
    lr: 0.1
    momentum: 0.9
    nesterov: False
    weight_decay: 0.0001
)
Restart from checkpoint output/google_commands/LambdaResnet_1D_mel_10/checkpoint.pth.tar
Starting the Training loop ...
    Number of Epochs: 250
    Batch size: 256
    Total batches: 331.42
Epoch 13/250
Adjusted learning rate to 0.09933
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6639201641082764
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6411033868789673
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6758761405944824
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6689079999923706
Progress: 12.35% 	 Batch: 41 	 Loss: 1.661159873008728
Progress: 15.36% 	 Batch: 51 	 Loss: 1.7041376829147339
Progress: 18.37% 	 Batch: 61 	 Loss: 1.674238920211792
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6684515476226807
Progress: 24.40% 	 Batch: 81 	 Loss: 1.7053968906402588
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6817071437835693
Progress: 30.42% 	 Batch: 101 	 Loss: 1.7051230669021606
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6456695795059204
Progress: 36.45% 	 Batch: 121 	 Loss: 1.672145962715149
Progress: 39.46% 	 Batch: 131 	 Loss: 1.670356035232544
Progress: 42.47% 	 Batch: 141 	 Loss: 1.683874487876892
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6860692501068115
Progress: 48.49% 	 Batch: 161 	 Loss: 1.690128207206726
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6693531274795532
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6865360736846924
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6877996921539307
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6623069047927856
Progress: 63.55% 	 Batch: 211 	 Loss: 1.7182648181915283
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6530627012252808
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6839088201522827
Progress: 72.59% 	 Batch: 241 	 Loss: 1.669270396232605
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6989693641662598
Progress: 78.61% 	 Batch: 261 	 Loss: 1.688668131828308
Progress: 81.63% 	 Batch: 271 	 Loss: 1.7206326723098755
Progress: 84.64% 	 Batch: 281 	 Loss: 1.71363365650177
Progress: 87.65% 	 Batch: 291 	 Loss: 1.7046788930892944
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6797256469726562
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6915122270584106
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6956523656845093
Progress: 99.70% 	 Batch: 331 	 Loss: 1.694512128829956
Validation Loss: 2.1434536469288363
Accuracy of the model (Test Data): 72.01%
Saving the most accurate current model for the Test dataset
Checkpoint ...
Epoch 14/250
Adjusted learning rate to 0.09923
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6765661239624023
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6823829412460327
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6636013984680176
Progress: 9.34% 	 Batch: 31 	 Loss: 1.686219573020935
Progress: 12.35% 	 Batch: 41 	 Loss: 1.7010098695755005
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6656298637390137
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6713933944702148
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6325035095214844
Progress: 24.40% 	 Batch: 81 	 Loss: 1.681645154953003
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6493024826049805
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6705187559127808
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6521199941635132
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6552189588546753
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6510741710662842
Progress: 42.47% 	 Batch: 141 	 Loss: 1.7127773761749268
Progress: 45.48% 	 Batch: 151 	 Loss: 1.678918480873108
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6714279651641846
Progress: 51.51% 	 Batch: 171 	 Loss: 1.653525471687317
Progress: 54.52% 	 Batch: 181 	 Loss: 1.678514838218689
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6697099208831787
Progress: 60.54% 	 Batch: 201 	 Loss: 1.7018630504608154
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6531604528427124
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6864973306655884
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6919419765472412
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6956965923309326
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6668667793273926
Progress: 78.61% 	 Batch: 261 	 Loss: 1.655885100364685
Progress: 81.63% 	 Batch: 271 	 Loss: 1.653857707977295
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6806799173355103
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6664069890975952
Progress: 90.66% 	 Batch: 301 	 Loss: 1.69996178150177
Progress: 93.67% 	 Batch: 311 	 Loss: 1.7017325162887573
Progress: 96.69% 	 Batch: 321 	 Loss: 1.7006065845489502
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6666231155395508
Validation Loss: 2.1077124705681434
Accuracy of the model (Test Data): 74.44%
Saving the most accurate current model for the Test dataset
Checkpoint ...
Epoch 15/250
Adjusted learning rate to 0.09912
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6894917488098145
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6936320066452026
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6891366243362427
Progress: 9.34% 	 Batch: 31 	 Loss: 1.7109501361846924
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6590059995651245
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6972272396087646
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6613593101501465
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6562703847885132
Progress: 24.40% 	 Batch: 81 	 Loss: 1.7099264860153198
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6866633892059326
Progress: 30.42% 	 Batch: 101 	 Loss: 1.66035795211792
Progress: 33.43% 	 Batch: 111 	 Loss: 1.7204424142837524
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6785486936569214
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6400822401046753
Progress: 42.47% 	 Batch: 141 	 Loss: 1.688334584236145
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6645041704177856
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6928033828735352
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6489086151123047
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6766979694366455
Progress: 57.53% 	 Batch: 191 	 Loss: 1.671549677848816
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6487077474594116
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6525827646255493
Progress: 66.57% 	 Batch: 221 	 Loss: 1.7043461799621582
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6809390783309937
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6875419616699219
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6714062690734863
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6731135845184326
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6800516843795776
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6738193035125732
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6775506734848022
Progress: 90.66% 	 Batch: 301 	 Loss: 1.7179392576217651
Progress: 93.67% 	 Batch: 311 	 Loss: 1.681433916091919
Progress: 96.69% 	 Batch: 321 	 Loss: 1.670338749885559
Progress: 99.70% 	 Batch: 331 	 Loss: 1.7287858724594116
Validation Loss: 2.1554901477618094
Accuracy of the model (Test Data): 72.99%
Checkpoint ...
Epoch 16/250
Adjusted learning rate to 0.09899
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6699756383895874
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6444294452667236
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6832960844039917
Progress: 9.34% 	 Batch: 31 	 Loss: 1.687109112739563
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6973121166229248
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6817368268966675
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6902693510055542
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6590855121612549
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6867860555648804
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6572339534759521
Progress: 30.42% 	 Batch: 101 	 Loss: 1.7076547145843506
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6682919263839722
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6721415519714355
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6609019041061401
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6806994676589966
Progress: 45.48% 	 Batch: 151 	 Loss: 1.681978464126587
Progress: 48.49% 	 Batch: 161 	 Loss: 1.720362901687622
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6347198486328125
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6959789991378784
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6751775741577148
Progress: 60.54% 	 Batch: 201 	 Loss: 1.7013744115829468
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6659497022628784
Progress: 66.57% 	 Batch: 221 	 Loss: 1.664048194885254
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6908268928527832
Progress: 72.59% 	 Batch: 241 	 Loss: 1.7300891876220703
Progress: 75.60% 	 Batch: 251 	 Loss: 1.680687427520752
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6749550104141235
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6710026264190674
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6558375358581543
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6627376079559326
Progress: 90.66% 	 Batch: 301 	 Loss: 1.7088425159454346
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6778045892715454
Progress: 96.69% 	 Batch: 321 	 Loss: 1.671248197555542
Progress: 99.70% 	 Batch: 331 	 Loss: 1.650758981704712
Validation Loss: 2.1897501945495605
Accuracy of the model (Test Data): 69.65%
Checkpoint ...
Epoch 17/250
Adjusted learning rate to 0.09886
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.7041317224502563
Progress: 3.31% 	 Batch: 11 	 Loss: 1.7344740629196167
Progress: 6.33% 	 Batch: 21 	 Loss: 1.711982250213623
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6747918128967285
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6848485469818115
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6497842073440552
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6899970769882202
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6795504093170166
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6533797979354858
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6697258949279785
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6384457349777222
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6662441492080688
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6697665452957153
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6572107076644897
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6814849376678467
Progress: 45.48% 	 Batch: 151 	 Loss: 1.690626621246338
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6892871856689453
Progress: 51.51% 	 Batch: 171 	 Loss: 1.7219114303588867
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6664570569992065
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6981860399246216
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6568241119384766
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6924138069152832
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6679540872573853
Progress: 69.58% 	 Batch: 231 	 Loss: 1.662786602973938
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6856815814971924
Progress: 75.60% 	 Batch: 251 	 Loss: 1.7120548486709595
Progress: 78.61% 	 Batch: 261 	 Loss: 1.650617241859436
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6666606664657593
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6450213193893433
Progress: 87.65% 	 Batch: 291 	 Loss: 1.672460675239563
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6557443141937256
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6904445886611938
Progress: 96.69% 	 Batch: 321 	 Loss: 1.7096898555755615
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6848284006118774
Validation Loss: 2.177949825922648
Accuracy of the model (Test Data): 68.73%
Checkpoint ...
Epoch 18/250
Adjusted learning rate to 0.09873
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.620296835899353
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6753315925598145
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6569217443466187
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6930339336395264
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6883413791656494
Progress: 15.36% 	 Batch: 51 	 Loss: 1.675653100013733
Progress: 18.37% 	 Batch: 61 	 Loss: 1.66702139377594
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6884195804595947
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6637126207351685
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6143848896026611
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6862221956253052
Progress: 33.43% 	 Batch: 111 	 Loss: 1.676360845565796
Progress: 36.45% 	 Batch: 121 	 Loss: 1.709864616394043
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6530836820602417
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6651719808578491
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6731247901916504
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6779643297195435
Progress: 51.51% 	 Batch: 171 	 Loss: 1.690974235534668
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6677076816558838
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6855336427688599
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6877816915512085
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6890755891799927
Progress: 66.57% 	 Batch: 221 	 Loss: 1.681701898574829
Progress: 69.58% 	 Batch: 231 	 Loss: 1.682814121246338
Progress: 72.59% 	 Batch: 241 	 Loss: 1.662354826927185
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6704472303390503
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6688112020492554
Progress: 81.63% 	 Batch: 271 	 Loss: 1.669959306716919
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6802932024002075
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6780365705490112
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6526330709457397
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6618562936782837
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6614922285079956
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6783045530319214
Validation Loss: 2.1744171044765372
Accuracy of the model (Test Data): 71.80%
Checkpoint ...
Epoch 19/250
Adjusted learning rate to 0.09858
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6938141584396362
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6522449254989624
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6757389307022095
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6805981397628784
Progress: 12.35% 	 Batch: 41 	 Loss: 1.658224105834961
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6496564149856567
Progress: 18.37% 	 Batch: 61 	 Loss: 1.666789174079895
Progress: 21.39% 	 Batch: 71 	 Loss: 1.7089041471481323
Progress: 24.40% 	 Batch: 81 	 Loss: 1.665177822113037
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6856292486190796
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6661710739135742
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6687544584274292
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6650197505950928
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6832903623580933
Progress: 42.47% 	 Batch: 141 	 Loss: 1.683298110961914
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6762588024139404
Progress: 48.49% 	 Batch: 161 	 Loss: 1.672749400138855
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6541938781738281
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6902070045471191
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6649805307388306
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6919424533843994
Progress: 63.55% 	 Batch: 211 	 Loss: 1.677561640739441
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6755422353744507
Progress: 69.58% 	 Batch: 231 	 Loss: 1.676000714302063
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6489359140396118
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6752961874008179
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6739298105239868
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6726081371307373
Progress: 84.64% 	 Batch: 281 	 Loss: 1.687760591506958
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6854897737503052
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6808170080184937
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6755739450454712
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6667858362197876
Progress: 99.70% 	 Batch: 331 	 Loss: 1.680949330329895
Validation Loss: 2.1370674188320455
Accuracy of the model (Test Data): 74.17%
Checkpoint ...
Epoch 20/250
Adjusted learning rate to 0.09843
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6898530721664429
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6351044178009033
Progress: 6.33% 	 Batch: 21 	 Loss: 1.654614806175232
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6772961616516113
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6417884826660156
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6877481937408447
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6782724857330322
Progress: 21.39% 	 Batch: 71 	 Loss: 1.675051212310791
Progress: 24.40% 	 Batch: 81 	 Loss: 1.69303560256958
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6444894075393677
Progress: 30.42% 	 Batch: 101 	 Loss: 1.67832350730896
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6674318313598633
Progress: 36.45% 	 Batch: 121 	 Loss: 1.669309139251709
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6677719354629517
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6979732513427734
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6640225648880005
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6702961921691895
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6978330612182617
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6538058519363403
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6489195823669434
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6435216665267944
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6583912372589111
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6664621829986572
Progress: 69.58% 	 Batch: 231 	 Loss: 1.652533769607544
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6729145050048828
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6543055772781372
Progress: 78.61% 	 Batch: 261 	 Loss: 1.657550573348999
Progress: 81.63% 	 Batch: 271 	 Loss: 1.67959725856781
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6980750560760498
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6520215272903442
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6720679998397827
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6511032581329346
Progress: 96.69% 	 Batch: 321 	 Loss: 1.7001147270202637
Progress: 99.70% 	 Batch: 331 	 Loss: 1.7077436447143555
Validation Loss: 2.176000353617546
Accuracy of the model (Test Data): 72.12%
Checkpoint ...
Epoch 21/250
Adjusted learning rate to 0.09827
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6539063453674316
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6677130460739136
Progress: 6.33% 	 Batch: 21 	 Loss: 1.676264762878418
Progress: 9.34% 	 Batch: 31 	 Loss: 1.633043646812439
Progress: 12.35% 	 Batch: 41 	 Loss: 1.7145228385925293
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6776221990585327
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6645100116729736
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6826571226119995
Progress: 24.40% 	 Batch: 81 	 Loss: 1.66013503074646
Progress: 27.41% 	 Batch: 91 	 Loss: 1.669814109802246
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6776853799819946
Progress: 33.43% 	 Batch: 111 	 Loss: 1.69227135181427
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6543482542037964
Progress: 39.46% 	 Batch: 131 	 Loss: 1.65137779712677
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6539275646209717
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6662967205047607
Progress: 48.49% 	 Batch: 161 	 Loss: 1.7093243598937988
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6633027791976929
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6460740566253662
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6744439601898193
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6849764585494995
Progress: 63.55% 	 Batch: 211 	 Loss: 1.670000433921814
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6654351949691772
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6760945320129395
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6465513706207275
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6115847826004028
Progress: 78.61% 	 Batch: 261 	 Loss: 1.650835394859314
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6862272024154663
Progress: 84.64% 	 Batch: 281 	 Loss: 1.744839072227478
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6562273502349854
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6804895401000977
Progress: 93.67% 	 Batch: 311 	 Loss: 1.657118797302246
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6479753255844116
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6684056520462036
Validation Loss: 2.160095758927174
Accuracy of the model (Test Data): 71.19%
Checkpoint ...
Epoch 22/250
Adjusted learning rate to 0.09810
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6958084106445312
Progress: 3.31% 	 Batch: 11 	 Loss: 1.645866870880127
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6344246864318848
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6723833084106445
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6268113851547241
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6643027067184448
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6805542707443237
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6340199708938599
Progress: 24.40% 	 Batch: 81 	 Loss: 1.7277522087097168
Progress: 27.41% 	 Batch: 91 	 Loss: 1.655709147453308
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6638052463531494
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6927646398544312
Progress: 36.45% 	 Batch: 121 	 Loss: 1.661097526550293
Progress: 39.46% 	 Batch: 131 	 Loss: 1.666054368019104
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6659493446350098
Progress: 45.48% 	 Batch: 151 	 Loss: 1.7100067138671875
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6754924058914185
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6622235774993896
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6725761890411377
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6737244129180908
Progress: 60.54% 	 Batch: 201 	 Loss: 1.7049720287322998
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6812057495117188
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6842137575149536
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6605478525161743
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6501309871673584
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6685158014297485
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6670688390731812
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6618839502334595
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6520782709121704
Progress: 87.65% 	 Batch: 291 	 Loss: 1.68960440158844
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6699953079223633
Progress: 93.67% 	 Batch: 311 	 Loss: 1.636650562286377
Progress: 96.69% 	 Batch: 321 	 Loss: 1.7058556079864502
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6841322183609009
Validation Loss: 2.156651475490668
Accuracy of the model (Test Data): 74.10%
Checkpoint ...
Epoch 23/250
Adjusted learning rate to 0.09793
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6666114330291748
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6634422540664673
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6471149921417236
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6301158666610718
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6598654985427856
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6722387075424194
Progress: 18.37% 	 Batch: 61 	 Loss: 1.660123586654663
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6558995246887207
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6872559785842896
Progress: 27.41% 	 Batch: 91 	 Loss: 1.65404212474823
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6695541143417358
Progress: 33.43% 	 Batch: 111 	 Loss: 1.7159978151321411
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6658036708831787
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6613256931304932
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6573185920715332
Progress: 45.48% 	 Batch: 151 	 Loss: 1.7018738985061646
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6770302057266235
Progress: 51.51% 	 Batch: 171 	 Loss: 1.7240005731582642
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6967830657958984
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6393415927886963
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6960591077804565
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6448755264282227
Progress: 66.57% 	 Batch: 221 	 Loss: 1.648516058921814
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6919649839401245
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6622114181518555
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6686216592788696
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6595754623413086
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6586918830871582
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6644952297210693
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6635671854019165
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6673935651779175
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6517164707183838
Progress: 96.69% 	 Batch: 321 	 Loss: 1.7078512907028198
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6835894584655762
Validation Loss: 2.1762236754099527
Accuracy of the model (Test Data): 69.61%
Checkpoint ...
Epoch 24/250
Adjusted learning rate to 0.09775
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6482856273651123
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6666388511657715
Progress: 6.33% 	 Batch: 21 	 Loss: 1.670978307723999
Progress: 9.34% 	 Batch: 31 	 Loss: 1.7001503705978394
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6736854314804077
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6774101257324219
Progress: 18.37% 	 Batch: 61 	 Loss: 1.633255124092102
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6702635288238525
Progress: 24.40% 	 Batch: 81 	 Loss: 1.671277642250061
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6669905185699463
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6856215000152588
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6375253200531006
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6647183895111084
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6330223083496094
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6599220037460327
Progress: 45.48% 	 Batch: 151 	 Loss: 1.644777774810791
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6618821620941162
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6877996921539307
Progress: 54.52% 	 Batch: 181 	 Loss: 1.662616491317749
Progress: 57.53% 	 Batch: 191 	 Loss: 1.719118356704712
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6733133792877197
Progress: 63.55% 	 Batch: 211 	 Loss: 1.664839506149292
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6532930135726929
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6605777740478516
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6827186346054077
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6751490831375122
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6952483654022217
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6734493970870972
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6710792779922485
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6949864625930786
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6912416219711304
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6903083324432373
Progress: 96.69% 	 Batch: 321 	 Loss: 1.669244408607483
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6374748945236206
Validation Loss: 2.1642029132598486
Accuracy of the model (Test Data): 72.82%
Checkpoint ...
Epoch 25/250
Adjusted learning rate to 0.09756
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6765754222869873
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6813626289367676
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6860990524291992
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6716684103012085
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6738227605819702
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6367766857147217
Progress: 18.37% 	 Batch: 61 	 Loss: 1.7050755023956299
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6978145837783813
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6651151180267334
Progress: 27.41% 	 Batch: 91 	 Loss: 1.729291319847107
Progress: 30.42% 	 Batch: 101 	 Loss: 1.7038626670837402
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6666606664657593
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6867250204086304
Progress: 39.46% 	 Batch: 131 	 Loss: 1.7186187505722046
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6701219081878662
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6373487710952759
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6506249904632568
Progress: 51.51% 	 Batch: 171 	 Loss: 1.681031346321106
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6354079246520996
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6475245952606201
Progress: 60.54% 	 Batch: 201 	 Loss: 1.691788911819458
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6371102333068848
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6512722969055176
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6345471143722534
Progress: 72.59% 	 Batch: 241 	 Loss: 1.709525465965271
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6472735404968262
Progress: 78.61% 	 Batch: 261 	 Loss: 1.682413935661316
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6447889804840088
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6731051206588745
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6788076162338257
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6542723178863525
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6244254112243652
Progress: 96.69% 	 Batch: 321 	 Loss: 1.690027117729187
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6406093835830688
Validation Loss: 2.1634308894475303
Accuracy of the model (Test Data): 74.54%
Saving the most accurate current model for the Test dataset
Checkpoint ...
Epoch 26/250
Adjusted learning rate to 0.09736
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6873160600662231
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6785531044006348
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6348096132278442
Progress: 9.34% 	 Batch: 31 	 Loss: 1.657097578048706
Progress: 12.35% 	 Batch: 41 	 Loss: 1.660140037536621
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6663203239440918
Progress: 18.37% 	 Batch: 61 	 Loss: 1.63311767578125
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6651802062988281
Progress: 24.40% 	 Batch: 81 	 Loss: 1.647668719291687
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6576097011566162
Progress: 30.42% 	 Batch: 101 	 Loss: 1.666129231452942
Progress: 33.43% 	 Batch: 111 	 Loss: 1.695412278175354
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6682735681533813
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6709564924240112
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6629140377044678
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6841384172439575
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6604437828063965
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6671313047409058
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6613527536392212
Progress: 57.53% 	 Batch: 191 	 Loss: 1.694913625717163
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6531938314437866
Progress: 63.55% 	 Batch: 211 	 Loss: 1.673905611038208
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6760449409484863
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6663724184036255
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6775689125061035
Progress: 75.60% 	 Batch: 251 	 Loss: 1.7055360078811646
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6645011901855469
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6537220478057861
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6511237621307373
Progress: 87.65% 	 Batch: 291 	 Loss: 1.688149333000183
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6805708408355713
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6687227487564087
Progress: 96.69% 	 Batch: 321 	 Loss: 1.7104296684265137
Progress: 99.70% 	 Batch: 331 	 Loss: 1.655586838722229
Validation Loss: 2.1607825603240576
Accuracy of the model (Test Data): 70.93%
Checkpoint ...
Epoch 27/250
Adjusted learning rate to 0.09715
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.679168939590454
Progress: 3.31% 	 Batch: 11 	 Loss: 1.655840516090393
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6678038835525513
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6709338426589966
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6890144348144531
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6565465927124023
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6709316968917847
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6587917804718018
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6817196607589722
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6534113883972168
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6711602210998535
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6676350831985474
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6548354625701904
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6573009490966797
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6664799451828003
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6666399240493774
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6603477001190186
Progress: 51.51% 	 Batch: 171 	 Loss: 1.648058533668518
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6529595851898193
Progress: 57.53% 	 Batch: 191 	 Loss: 1.624833583831787
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6692168712615967
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6797906160354614
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6751025915145874
Progress: 69.58% 	 Batch: 231 	 Loss: 1.672526240348816
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6757535934448242
Progress: 75.60% 	 Batch: 251 	 Loss: 1.69975745677948
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6775164604187012
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6627328395843506
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6619222164154053
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6682106256484985
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6353001594543457
Progress: 93.67% 	 Batch: 311 	 Loss: 1.66923987865448
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6918971538543701
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6505483388900757
Validation Loss: 2.149166452579009
Accuracy of the model (Test Data): 72.48%
Checkpoint ...
Epoch 28/250
Adjusted learning rate to 0.09694
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.666017770767212
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6735039949417114
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6404438018798828
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6336257457733154
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6898550987243652
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6213711500167847
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6808408498764038
Progress: 21.39% 	 Batch: 71 	 Loss: 1.667706847190857
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6680132150650024
Progress: 27.41% 	 Batch: 91 	 Loss: 1.632869839668274
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6901235580444336
Progress: 33.43% 	 Batch: 111 	 Loss: 1.648398756980896
Progress: 36.45% 	 Batch: 121 	 Loss: 1.638425350189209
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6684852838516235
Progress: 42.47% 	 Batch: 141 	 Loss: 1.660664439201355
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6593338251113892
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6515556573867798
Progress: 51.51% 	 Batch: 171 	 Loss: 1.697140097618103
Progress: 54.52% 	 Batch: 181 	 Loss: 1.612608551979065
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6891039609909058
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6477116346359253
Progress: 63.55% 	 Batch: 211 	 Loss: 1.66002357006073
Progress: 66.57% 	 Batch: 221 	 Loss: 1.648103952407837
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6455539464950562
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6918991804122925
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6273705959320068
Progress: 78.61% 	 Batch: 261 	 Loss: 1.698813796043396
Progress: 81.63% 	 Batch: 271 	 Loss: 1.680528998374939
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6892622709274292
Progress: 87.65% 	 Batch: 291 	 Loss: 1.635561227798462
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6571682691574097
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6663426160812378
Progress: 96.69% 	 Batch: 321 	 Loss: 1.679488182067871
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6630560159683228
Validation Loss: 2.1201524979028945
Accuracy of the model (Test Data): 75.21%
Saving the most accurate current model for the Test dataset
Checkpoint ...
Epoch 29/250
Adjusted learning rate to 0.09672
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6391083002090454
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6843926906585693
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6487083435058594
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6265166997909546
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6611583232879639
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6702804565429688
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6584587097167969
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6592280864715576
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6703755855560303
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6794352531433105
Progress: 30.42% 	 Batch: 101 	 Loss: 1.662606120109558
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6260191202163696
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6618447303771973
Progress: 39.46% 	 Batch: 131 	 Loss: 1.688565969467163
Progress: 42.47% 	 Batch: 141 	 Loss: 1.66664719581604
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6594011783599854
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6582059860229492
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6786102056503296
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6752209663391113
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6506866216659546
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6453477144241333
Progress: 63.55% 	 Batch: 211 	 Loss: 1.7092031240463257
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6756505966186523
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6476759910583496
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6684303283691406
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6625157594680786
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6656196117401123
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6428165435791016
Progress: 84.64% 	 Batch: 281 	 Loss: 1.665785789489746
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6383377313613892
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6491281986236572
Progress: 93.67% 	 Batch: 311 	 Loss: 1.670426607131958
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6269989013671875
Progress: 99.70% 	 Batch: 331 	 Loss: 1.7141878604888916
Validation Loss: 2.2337908347447715
Accuracy of the model (Test Data): 71.57%
Checkpoint ...
Epoch 30/250
Adjusted learning rate to 0.09649
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.662484049797058
Progress: 3.31% 	 Batch: 11 	 Loss: 1.655681848526001
Progress: 6.33% 	 Batch: 21 	 Loss: 1.666723608970642
Progress: 9.34% 	 Batch: 31 	 Loss: 1.669996738433838
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6814703941345215
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6560792922973633
Progress: 18.37% 	 Batch: 61 	 Loss: 1.651092290878296
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6092088222503662
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6391713619232178
Progress: 27.41% 	 Batch: 91 	 Loss: 1.665583610534668
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6533746719360352
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6583706140518188
Progress: 36.45% 	 Batch: 121 	 Loss: 1.656509280204773
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6699298620224
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6618537902832031
Progress: 45.48% 	 Batch: 151 	 Loss: 1.665867567062378
Progress: 48.49% 	 Batch: 161 	 Loss: 1.66121506690979
Progress: 51.51% 	 Batch: 171 	 Loss: 1.642877459526062
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6539669036865234
Progress: 57.53% 	 Batch: 191 	 Loss: 1.668121576309204
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6718116998672485
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6454553604125977
Progress: 66.57% 	 Batch: 221 	 Loss: 1.664736032485962
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6491085290908813
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6670793294906616
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6650006771087646
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6474887132644653
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6530381441116333
Progress: 84.64% 	 Batch: 281 	 Loss: 1.653348684310913
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6841965913772583
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6590869426727295
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6517333984375
Progress: 96.69% 	 Batch: 321 	 Loss: 1.663046956062317
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6578621864318848
Validation Loss: 2.1635740720308743
Accuracy of the model (Test Data): 74.17%
Checkpoint ...
Epoch 31/250
Adjusted learning rate to 0.09626
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6261134147644043
Progress: 3.31% 	 Batch: 11 	 Loss: 1.675499439239502
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6430583000183105
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6314493417739868
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6635172367095947
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6605762243270874
Progress: 18.37% 	 Batch: 61 	 Loss: 1.670898199081421
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6389175653457642
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6703599691390991
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6552557945251465
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6243038177490234
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6434720754623413
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6452221870422363
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6592826843261719
Progress: 42.47% 	 Batch: 141 	 Loss: 1.669735074043274
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6585094928741455
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6310821771621704
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6202805042266846
Progress: 54.52% 	 Batch: 181 	 Loss: 1.708771824836731
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6868841648101807
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6629964113235474
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6854758262634277
Progress: 66.57% 	 Batch: 221 	 Loss: 1.66806161403656
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6408437490463257
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6393283605575562
Progress: 75.60% 	 Batch: 251 	 Loss: 1.674891710281372
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6325697898864746
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6473214626312256
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6416722536087036
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6516155004501343
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6815235614776611
Progress: 93.67% 	 Batch: 311 	 Loss: 1.669848084449768
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6242003440856934
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6520366668701172
Validation Loss: 2.1941842696605582
Accuracy of the model (Test Data): 72.46%
Checkpoint ...
Epoch 32/250
Adjusted learning rate to 0.09602
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6528788805007935
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6873784065246582
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6718777418136597
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6532779932022095
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6692578792572021
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6527289152145386
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6922526359558105
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6826977729797363
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6677449941635132
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6356947422027588
Progress: 30.42% 	 Batch: 101 	 Loss: 1.634247899055481
Progress: 33.43% 	 Batch: 111 	 Loss: 1.665921688079834
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6487653255462646
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6776033639907837
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6910902261734009
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6963598728179932
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6680028438568115
Progress: 51.51% 	 Batch: 171 	 Loss: 1.679233193397522
Progress: 54.52% 	 Batch: 181 	 Loss: 1.680757761001587
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6709734201431274
Progress: 60.54% 	 Batch: 201 	 Loss: 1.647799015045166
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6797325611114502
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6362600326538086
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6673849821090698
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6625540256500244
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6775527000427246
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6340322494506836
Progress: 81.63% 	 Batch: 271 	 Loss: 1.668375849723816
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6820486783981323
Progress: 87.65% 	 Batch: 291 	 Loss: 1.643008828163147
Progress: 90.66% 	 Batch: 301 	 Loss: 1.63260817527771
Progress: 93.67% 	 Batch: 311 	 Loss: 1.633965015411377
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6560389995574951
Progress: 99.70% 	 Batch: 331 	 Loss: 1.681623101234436
Validation Loss: 2.1740158521212063
Accuracy of the model (Test Data): 70.70%
Checkpoint ...
Epoch 33/250
Adjusted learning rate to 0.09577
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6609987020492554
Progress: 3.31% 	 Batch: 11 	 Loss: 1.658912181854248
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6704635620117188
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6211918592453003
Progress: 12.35% 	 Batch: 41 	 Loss: 1.7081849575042725
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6656399965286255
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6497392654418945
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6736701726913452
Progress: 24.40% 	 Batch: 81 	 Loss: 1.689659595489502
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6668403148651123
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6825426816940308
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6740729808807373
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6294370889663696
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6399039030075073
Progress: 42.47% 	 Batch: 141 	 Loss: 1.655478596687317
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6729769706726074
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6514084339141846
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6356664896011353
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6692111492156982
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6658326387405396
Progress: 60.54% 	 Batch: 201 	 Loss: 1.7139121294021606
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6541048288345337
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6469191312789917
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6708563566207886
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6345288753509521
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6524326801300049
Progress: 78.61% 	 Batch: 261 	 Loss: 1.619354248046875
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6382685899734497
Progress: 84.64% 	 Batch: 281 	 Loss: 1.668904185295105
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6408579349517822
Progress: 90.66% 	 Batch: 301 	 Loss: 1.650141954421997
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6470601558685303
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6939741373062134
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6238799095153809
Validation Loss: 2.14107922407297
Accuracy of the model (Test Data): 76.36%
Saving the most accurate current model for the Test dataset
Checkpoint ...
Epoch 34/250
Adjusted learning rate to 0.09551
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6515421867370605
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6624867916107178
Progress: 6.33% 	 Batch: 21 	 Loss: 1.680835247039795
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6584365367889404
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6591382026672363
Progress: 15.36% 	 Batch: 51 	 Loss: 1.677520513534546
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6381851434707642
Progress: 21.39% 	 Batch: 71 	 Loss: 1.661962866783142
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6397345066070557
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6405258178710938
Progress: 30.42% 	 Batch: 101 	 Loss: 1.612728238105774
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6540583372116089
Progress: 36.45% 	 Batch: 121 	 Loss: 1.661421298980713
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6515450477600098
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6482245922088623
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6574605703353882
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6641860008239746
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6636933088302612
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6474260091781616
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6547012329101562
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6439250707626343
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6712464094161987
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6521897315979004
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6608449220657349
Progress: 72.59% 	 Batch: 241 	 Loss: 1.637677788734436
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6554784774780273
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6685775518417358
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6423836946487427
Progress: 84.64% 	 Batch: 281 	 Loss: 1.664639949798584
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6731373071670532
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6649616956710815
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6633496284484863
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6803265810012817
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6613701581954956
Validation Loss: 2.1668679072306705
Accuracy of the model (Test Data): 71.21%
Checkpoint ...
Epoch 35/250
Adjusted learning rate to 0.09525
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6533221006393433
Progress: 3.31% 	 Batch: 11 	 Loss: 1.645233154296875
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6418949365615845
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6495797634124756
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6564522981643677
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6418243646621704
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6955167055130005
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6707167625427246
Progress: 24.40% 	 Batch: 81 	 Loss: 1.7089892625808716
Progress: 27.41% 	 Batch: 91 	 Loss: 1.604047179222107
Progress: 30.42% 	 Batch: 101 	 Loss: 1.66251540184021
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6612919569015503
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6577867269515991
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6729326248168945
Progress: 42.47% 	 Batch: 141 	 Loss: 1.681251049041748
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6573164463043213
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6612356901168823
Progress: 51.51% 	 Batch: 171 	 Loss: 1.652992606163025
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6317768096923828
Progress: 57.53% 	 Batch: 191 	 Loss: 1.624402642250061
Progress: 60.54% 	 Batch: 201 	 Loss: 1.639603614807129
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6856032609939575
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6380069255828857
Progress: 69.58% 	 Batch: 231 	 Loss: 1.647359013557434
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6653120517730713
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6211473941802979
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6685445308685303
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6683123111724854
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6414469480514526
Progress: 87.65% 	 Batch: 291 	 Loss: 1.697436809539795
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6472021341323853
Progress: 93.67% 	 Batch: 311 	 Loss: 1.680003046989441
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6604547500610352
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6520823240280151
Validation Loss: 2.1345037863804746
Accuracy of the model (Test Data): 74.75%
Checkpoint ...
Epoch 36/250
Adjusted learning rate to 0.09498
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6367928981781006
Progress: 3.31% 	 Batch: 11 	 Loss: 1.646962285041809
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6386536359786987
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6482317447662354
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6436386108398438
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6356384754180908
Progress: 18.37% 	 Batch: 61 	 Loss: 1.663031816482544
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6339874267578125
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6731704473495483
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6520469188690186
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6935882568359375
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6596192121505737
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6644539833068848
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6566389799118042
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6488561630249023
Progress: 45.48% 	 Batch: 151 	 Loss: 1.645087480545044
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6506679058074951
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6938384771347046
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6799241304397583
Progress: 57.53% 	 Batch: 191 	 Loss: 1.663297176361084
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6612197160720825
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6335792541503906
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6463818550109863
Progress: 69.58% 	 Batch: 231 	 Loss: 1.626690149307251
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6520711183547974
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6476277112960815
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6536165475845337
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6684200763702393
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6545028686523438
Progress: 87.65% 	 Batch: 291 	 Loss: 1.688408613204956
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6581889390945435
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6402254104614258
Progress: 96.69% 	 Batch: 321 	 Loss: 1.655529260635376
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6479960680007935
Validation Loss: 2.1612323002937512
Accuracy of the model (Test Data): 72.12%
Checkpoint ...
Epoch 37/250
Adjusted learning rate to 0.09470
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6421284675598145
Progress: 3.31% 	 Batch: 11 	 Loss: 1.5981054306030273
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6363099813461304
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6623902320861816
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6855671405792236
Progress: 15.36% 	 Batch: 51 	 Loss: 1.65300452709198
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6551105976104736
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6645106077194214
Progress: 24.40% 	 Batch: 81 	 Loss: 1.638569712638855
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6744359731674194
Progress: 30.42% 	 Batch: 101 	 Loss: 1.656158447265625
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6800447702407837
Progress: 36.45% 	 Batch: 121 	 Loss: 1.642711877822876
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6436465978622437
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6548125743865967
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6421056985855103
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6462043523788452
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6667921543121338
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6438449621200562
Progress: 57.53% 	 Batch: 191 	 Loss: 1.642266869544983
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6312973499298096
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6554428339004517
Progress: 66.57% 	 Batch: 221 	 Loss: 1.683345913887024
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6683686971664429
Progress: 72.59% 	 Batch: 241 	 Loss: 1.64621102809906
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6568390130996704
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6532933712005615
Progress: 81.63% 	 Batch: 271 	 Loss: 1.623348355293274
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6528470516204834
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6747167110443115
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6768473386764526
Progress: 93.67% 	 Batch: 311 	 Loss: 1.660233736038208
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6762714385986328
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6725268363952637
Validation Loss: 2.169836490582197
Accuracy of the model (Test Data): 73.94%
Checkpoint ...
Epoch 38/250
Adjusted learning rate to 0.09441
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6776808500289917
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6648495197296143
Progress: 6.33% 	 Batch: 21 	 Loss: 1.664910912513733
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6620893478393555
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6380585432052612
Progress: 15.36% 	 Batch: 51 	 Loss: 1.670032024383545
Progress: 18.37% 	 Batch: 61 	 Loss: 1.679648756980896
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6396381855010986
Progress: 24.40% 	 Batch: 81 	 Loss: 1.634253740310669
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6762832403182983
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6716303825378418
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6268528699874878
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6654247045516968
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6416072845458984
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6325652599334717
Progress: 45.48% 	 Batch: 151 	 Loss: 1.669050693511963
Progress: 48.49% 	 Batch: 161 	 Loss: 1.651832938194275
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6404635906219482
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6682875156402588
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6486408710479736
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6108165979385376
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6821140050888062
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6575109958648682
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6537081003189087
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6855107545852661
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6680817604064941
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6506097316741943
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6381069421768188
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6478852033615112
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6661720275878906
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6718802452087402
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6620854139328003
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6240179538726807
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6755248308181763
Validation Loss: 2.131525253638243
Accuracy of the model (Test Data): 74.51%
Checkpoint ...
Epoch 39/250
Adjusted learning rate to 0.09412
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.671812891960144
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6833558082580566
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6580888032913208
Progress: 9.34% 	 Batch: 31 	 Loss: 1.632927417755127
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6279144287109375
Progress: 15.36% 	 Batch: 51 	 Loss: 1.624101996421814
Progress: 18.37% 	 Batch: 61 	 Loss: 1.666027307510376
Progress: 21.39% 	 Batch: 71 	 Loss: 1.657236933708191
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6166415214538574
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6280639171600342
Progress: 30.42% 	 Batch: 101 	 Loss: 1.623288631439209
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6246521472930908
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6127780675888062
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6463121175765991
Progress: 42.47% 	 Batch: 141 	 Loss: 1.689292311668396
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6388089656829834
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6461849212646484
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6361898183822632
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6746020317077637
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6504054069519043
Progress: 60.54% 	 Batch: 201 	 Loss: 1.682479739189148
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6361737251281738
Progress: 66.57% 	 Batch: 221 	 Loss: 1.630204439163208
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6444671154022217
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6562583446502686
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6702780723571777
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6127320528030396
Progress: 81.63% 	 Batch: 271 	 Loss: 1.67373526096344
Progress: 84.64% 	 Batch: 281 	 Loss: 1.654371738433838
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6494300365447998
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6382396221160889
Progress: 93.67% 	 Batch: 311 	 Loss: 1.627301573753357
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6518592834472656
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6456589698791504
Validation Loss: 2.1768593879846425
Accuracy of the model (Test Data): 72.86%
Checkpoint ...
Epoch 40/250
Adjusted learning rate to 0.09382
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6566572189331055
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6536816358566284
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6243019104003906
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6201910972595215
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6516799926757812
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6327155828475952
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6289387941360474
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6775025129318237
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6337294578552246
Progress: 27.41% 	 Batch: 91 	 Loss: 1.666551113128662
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6656442880630493
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6621798276901245
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6535141468048096
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6305320262908936
Progress: 42.47% 	 Batch: 141 	 Loss: 1.657637357711792
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6809849739074707
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6635816097259521
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6363943815231323
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6294138431549072
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6198389530181885
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6164882183074951
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6623053550720215
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6628103256225586
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6690807342529297
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6344302892684937
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6504228115081787
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6587039232254028
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6605135202407837
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6517606973648071
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6551300287246704
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6165950298309326
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6286836862564087
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6985503435134888
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6593265533447266
Validation Loss: 2.1463159139339743
Accuracy of the model (Test Data): 72.38%
Checkpoint ...
Epoch 41/250
Adjusted learning rate to 0.09352
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6543411016464233
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6609838008880615
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6500015258789062
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6608418226242065
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6525249481201172
Progress: 15.36% 	 Batch: 51 	 Loss: 1.654768943786621
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6437321901321411
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6560308933258057
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6620237827301025
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6472080945968628
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6681209802627563
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6731641292572021
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6347718238830566
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6357338428497314
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6231250762939453
Progress: 45.48% 	 Batch: 151 	 Loss: 1.637209415435791
Progress: 48.49% 	 Batch: 161 	 Loss: 1.674033761024475
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6651239395141602
Progress: 54.52% 	 Batch: 181 	 Loss: 1.647567868232727
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6492066383361816
Progress: 60.54% 	 Batch: 201 	 Loss: 1.673874855041504
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6680854558944702
Progress: 66.57% 	 Batch: 221 	 Loss: 1.617970585823059
Progress: 69.58% 	 Batch: 231 	 Loss: 1.692034363746643
Progress: 72.59% 	 Batch: 241 	 Loss: 1.643227219581604
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6332963705062866
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6370574235916138
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6334972381591797
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6711763143539429
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6536548137664795
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6419923305511475
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6400177478790283
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6475486755371094
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6291589736938477
Validation Loss: 2.146486936471401
Accuracy of the model (Test Data): 73.46%
Checkpoint ...
Epoch 42/250
Adjusted learning rate to 0.09320
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6670829057693481
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6593643426895142
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6293809413909912
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6224628686904907
Progress: 12.35% 	 Batch: 41 	 Loss: 1.649733066558838
Progress: 15.36% 	 Batch: 51 	 Loss: 1.66782808303833
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6422390937805176
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6721993684768677
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6413426399230957
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6435036659240723
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6387743949890137
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6515408754348755
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6557222604751587
Progress: 39.46% 	 Batch: 131 	 Loss: 1.631829857826233
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6513667106628418
Progress: 45.48% 	 Batch: 151 	 Loss: 1.638710856437683
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6259158849716187
Progress: 51.51% 	 Batch: 171 	 Loss: 1.614172339439392
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6614267826080322
Progress: 57.53% 	 Batch: 191 	 Loss: 1.667665958404541
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6797541379928589
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6525838375091553
Progress: 66.57% 	 Batch: 221 	 Loss: 1.655742883682251
Progress: 69.58% 	 Batch: 231 	 Loss: 1.614248275756836
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6548433303833008
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6847805976867676
Progress: 78.61% 	 Batch: 261 	 Loss: 1.647559642791748
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6285016536712646
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6706548929214478
Progress: 87.65% 	 Batch: 291 	 Loss: 1.634844183921814
Progress: 90.66% 	 Batch: 301 	 Loss: 1.640307903289795
Progress: 93.67% 	 Batch: 311 	 Loss: 1.663028597831726
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6384719610214233
Progress: 99.70% 	 Batch: 331 	 Loss: 1.645513653755188
Validation Loss: 2.184245066765027
Accuracy of the model (Test Data): 72.08%
Checkpoint ...
Epoch 43/250
Adjusted learning rate to 0.09288
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6347776651382446
Progress: 3.31% 	 Batch: 11 	 Loss: 1.655332088470459
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6385319232940674
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6350260972976685
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6499344110488892
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6157830953598022
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6309858560562134
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6317307949066162
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6096038818359375
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6430540084838867
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6648383140563965
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6346861124038696
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6419495344161987
Progress: 39.46% 	 Batch: 131 	 Loss: 1.644634485244751
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6465693712234497
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6196099519729614
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6621869802474976
Progress: 51.51% 	 Batch: 171 	 Loss: 1.7046518325805664
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6472256183624268
Progress: 57.53% 	 Batch: 191 	 Loss: 1.652571439743042
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6460292339324951
Progress: 63.55% 	 Batch: 211 	 Loss: 1.635425329208374
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6455286741256714
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6775399446487427
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6628540754318237
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6392031908035278
Progress: 78.61% 	 Batch: 261 	 Loss: 1.668619990348816
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6604722738265991
Progress: 84.64% 	 Batch: 281 	 Loss: 1.669824242591858
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6230868101119995
Progress: 90.66% 	 Batch: 301 	 Loss: 1.648686408996582
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6328831911087036
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6856472492218018
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6406269073486328
Validation Loss: 2.1455502540637283
Accuracy of the model (Test Data): 75.30%
Checkpoint ...
Epoch 44/250
Adjusted learning rate to 0.09256
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.65645170211792
Progress: 3.31% 	 Batch: 11 	 Loss: 1.619560718536377
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6487146615982056
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6389752626419067
Progress: 12.35% 	 Batch: 41 	 Loss: 1.635996699333191
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6808909177780151
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6438180208206177
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6524418592453003
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6607518196105957
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6356923580169678
Progress: 30.42% 	 Batch: 101 	 Loss: 1.64822256565094
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6634970903396606
Progress: 36.45% 	 Batch: 121 	 Loss: 1.615246295928955
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6462461948394775
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6595381498336792
Progress: 45.48% 	 Batch: 151 	 Loss: 1.646179437637329
Progress: 48.49% 	 Batch: 161 	 Loss: 1.7062771320343018
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6470998525619507
Progress: 54.52% 	 Batch: 181 	 Loss: 1.638238549232483
Progress: 57.53% 	 Batch: 191 	 Loss: 1.665899395942688
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6614748239517212
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6267809867858887
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6295979022979736
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6392805576324463
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6758081912994385
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6482115983963013
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6702632904052734
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6428711414337158
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6452511548995972
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6272916793823242
Progress: 90.66% 	 Batch: 301 	 Loss: 1.656017780303955
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6584985256195068
Progress: 96.69% 	 Batch: 321 	 Loss: 1.641948938369751
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6946015357971191
Validation Loss: 2.1508942536818676
Accuracy of the model (Test Data): 73.90%
Checkpoint ...
Epoch 45/250
Adjusted learning rate to 0.09222
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6300859451293945
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6004599332809448
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6510746479034424
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6525144577026367
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6395834684371948
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6435867547988892
Progress: 18.37% 	 Batch: 61 	 Loss: 1.657773494720459
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6625515222549438
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6372618675231934
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6300930976867676
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6404186487197876
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6395150423049927
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6407748460769653
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6564942598342896
Progress: 42.47% 	 Batch: 141 	 Loss: 1.648505687713623
Progress: 45.48% 	 Batch: 151 	 Loss: 1.646125078201294
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6583642959594727
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6742219924926758
Progress: 54.52% 	 Batch: 181 	 Loss: 1.678318738937378
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6394964456558228
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6307705640792847
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6522059440612793
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6409720182418823
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6453239917755127
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6723634004592896
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6384631395339966
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6503334045410156
Progress: 81.63% 	 Batch: 271 	 Loss: 1.63992178440094
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6713474988937378
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6648931503295898
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6314847469329834
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6620872020721436
Progress: 96.69% 	 Batch: 321 	 Loss: 1.627395510673523
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6463608741760254
Validation Loss: 2.157781876050509
Accuracy of the model (Test Data): 74.15%
Checkpoint ...
Epoch 46/250
Adjusted learning rate to 0.09188
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6396965980529785
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6491822004318237
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6547269821166992
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6606448888778687
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6559170484542847
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6797336339950562
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6414175033569336
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6692297458648682
Progress: 24.40% 	 Batch: 81 	 Loss: 1.681536078453064
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6698859930038452
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6330351829528809
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6744366884231567
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6628286838531494
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6390807628631592
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6544469594955444
Progress: 45.48% 	 Batch: 151 	 Loss: 1.652071475982666
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6429002285003662
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6365174055099487
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6491061449050903
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6590579748153687
Progress: 60.54% 	 Batch: 201 	 Loss: 1.701094627380371
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6592124700546265
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6673510074615479
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6488462686538696
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6717829704284668
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6238080263137817
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6528737545013428
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6660066843032837
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6393978595733643
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6670668125152588
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6570404767990112
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6672152280807495
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6718965768814087
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6369755268096924
Validation Loss: 2.1320400696534376
Accuracy of the model (Test Data): 75.37%
Checkpoint ...
Epoch 47/250
Adjusted learning rate to 0.09154
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.663447380065918
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6560537815093994
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6563879251480103
Progress: 9.34% 	 Batch: 31 	 Loss: 1.65142822265625
Progress: 12.35% 	 Batch: 41 	 Loss: 1.677182912826538
Progress: 15.36% 	 Batch: 51 	 Loss: 1.648954153060913
Progress: 18.37% 	 Batch: 61 	 Loss: 1.616611123085022
Progress: 21.39% 	 Batch: 71 	 Loss: 1.643871784210205
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6615047454833984
Progress: 27.41% 	 Batch: 91 	 Loss: 1.673819661140442
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6772959232330322
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6752934455871582
Progress: 36.45% 	 Batch: 121 	 Loss: 1.663683295249939
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6629184484481812
Progress: 42.47% 	 Batch: 141 	 Loss: 1.641692876815796
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6670162677764893
Progress: 48.49% 	 Batch: 161 	 Loss: 1.654172420501709
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6403735876083374
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6420773267745972
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6777626276016235
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6298295259475708
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6384410858154297
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6462944746017456
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6185901165008545
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6433409452438354
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6177663803100586
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6638654470443726
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6583330631256104
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6345895528793335
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6484811305999756
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6735177040100098
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6493606567382812
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6448583602905273
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6355394124984741
Validation Loss: 2.180708218843509
Accuracy of the model (Test Data): 72.79%
Checkpoint ...
Epoch 48/250
Adjusted learning rate to 0.09119
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6483447551727295
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6735453605651855
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6429095268249512
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6732194423675537
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6502189636230469
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6607093811035156
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6755423545837402
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6436028480529785
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6216139793395996
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6467515230178833
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6465736627578735
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6875215768814087
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6567232608795166
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6487400531768799
Progress: 42.47% 	 Batch: 141 	 Loss: 1.647902011871338
Progress: 45.48% 	 Batch: 151 	 Loss: 1.667686939239502
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6603646278381348
Progress: 51.51% 	 Batch: 171 	 Loss: 1.620841383934021
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6484555006027222
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6684060096740723
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6567846536636353
Progress: 63.55% 	 Batch: 211 	 Loss: 1.673411250114441
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6486093997955322
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6366050243377686
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6641788482666016
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6564112901687622
Progress: 78.61% 	 Batch: 261 	 Loss: 1.651068091392517
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6472148895263672
Progress: 84.64% 	 Batch: 281 	 Loss: 1.5929549932479858
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6774263381958008
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6344335079193115
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6374504566192627
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6900794506072998
Progress: 99.70% 	 Batch: 331 	 Loss: 1.663655161857605
Validation Loss: 2.1689399572519155
Accuracy of the model (Test Data): 75.30%
Checkpoint ...
Epoch 49/250
Adjusted learning rate to 0.09083
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6558260917663574
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6228570938110352
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6576759815216064
Progress: 9.34% 	 Batch: 31 	 Loss: 1.634971022605896
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6633877754211426
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6725702285766602
Progress: 18.37% 	 Batch: 61 	 Loss: 1.670695185661316
Progress: 21.39% 	 Batch: 71 	 Loss: 1.68234384059906
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6168380975723267
Progress: 27.41% 	 Batch: 91 	 Loss: 1.645957112312317
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6498644351959229
Progress: 33.43% 	 Batch: 111 	 Loss: 1.624463438987732
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6454039812088013
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6316661834716797
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6418919563293457
Progress: 45.48% 	 Batch: 151 	 Loss: 1.668843150138855
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6186153888702393
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6413015127182007
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6004574298858643
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6708868741989136
Progress: 60.54% 	 Batch: 201 	 Loss: 1.659314513206482
Progress: 63.55% 	 Batch: 211 	 Loss: 1.687226414680481
Progress: 66.57% 	 Batch: 221 	 Loss: 1.658829927444458
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6599496603012085
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6014325618743896
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6696698665618896
Progress: 78.61% 	 Batch: 261 	 Loss: 1.673924446105957
Progress: 81.63% 	 Batch: 271 	 Loss: 1.642310619354248
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6267203092575073
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6446938514709473
Progress: 90.66% 	 Batch: 301 	 Loss: 1.620913028717041
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6409822702407837
Progress: 96.69% 	 Batch: 321 	 Loss: 1.696397066116333
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6481049060821533
Validation Loss: 2.1979358349090967
Accuracy of the model (Test Data): 73.78%
Checkpoint ...
Epoch 50/250
Adjusted learning rate to 0.09046
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.669034719467163
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6538631916046143
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6721625328063965
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6235859394073486
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6396090984344482
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6272001266479492
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6657416820526123
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6502971649169922
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6457175016403198
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6770834922790527
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6615873575210571
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6161985397338867
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6392813920974731
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6413613557815552
Progress: 42.47% 	 Batch: 141 	 Loss: 1.678657054901123
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6295922994613647
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6484904289245605
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6432868242263794
Progress: 54.52% 	 Batch: 181 	 Loss: 1.641321063041687
Progress: 57.53% 	 Batch: 191 	 Loss: 1.668270468711853
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6473195552825928
Progress: 63.55% 	 Batch: 211 	 Loss: 1.630111575126648
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6360656023025513
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6290678977966309
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6709914207458496
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6580349206924438
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6188751459121704
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6328325271606445
Progress: 84.64% 	 Batch: 281 	 Loss: 1.626452088356018
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6815283298492432
Progress: 90.66% 	 Batch: 301 	 Loss: 1.619589924812317
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6447827816009521
Progress: 96.69% 	 Batch: 321 	 Loss: 1.647035837173462
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6431819200515747
Validation Loss: 2.214036382161654
Accuracy of the model (Test Data): 72.02%
Checkpoint ...
Epoch 51/250
Adjusted learning rate to 0.09009
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.663845181465149
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6228183507919312
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6502115726470947
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6607764959335327
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6552157402038574
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6395930051803589
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6502139568328857
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6650415658950806
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6395646333694458
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6308174133300781
Progress: 30.42% 	 Batch: 101 	 Loss: 1.612607479095459
Progress: 33.43% 	 Batch: 111 	 Loss: 1.649855613708496
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6642087697982788
Progress: 39.46% 	 Batch: 131 	 Loss: 1.639306664466858
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6246428489685059
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6435601711273193
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6465463638305664
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6499978303909302
Progress: 54.52% 	 Batch: 181 	 Loss: 1.648300290107727
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6641759872436523
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6690576076507568
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6498494148254395
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6422404050827026
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6577773094177246
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6375869512557983
Progress: 75.60% 	 Batch: 251 	 Loss: 1.625333309173584
Progress: 78.61% 	 Batch: 261 	 Loss: 1.661170482635498
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6571332216262817
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6872330904006958
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6491764783859253
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6452839374542236
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6509565114974976
Progress: 96.69% 	 Batch: 321 	 Loss: 1.658064842224121
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6511338949203491
Validation Loss: 2.1566125551859536
Accuracy of the model (Test Data): 73.98%
Checkpoint ...
Epoch 52/250
Adjusted learning rate to 0.08971
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6521419286727905
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6462812423706055
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6666877269744873
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6315886974334717
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6416420936584473
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6749564409255981
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6634163856506348
Progress: 21.39% 	 Batch: 71 	 Loss: 1.643656849861145
Progress: 24.40% 	 Batch: 81 	 Loss: 1.63319730758667
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6945563554763794
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6566334962844849
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6491429805755615
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6013436317443848
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6160924434661865
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6694104671478271
Progress: 45.48% 	 Batch: 151 	 Loss: 1.624043583869934
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6322345733642578
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6288421154022217
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6773298978805542
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6564275026321411
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6236815452575684
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6417946815490723
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6624653339385986
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6546730995178223
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6516551971435547
Progress: 75.60% 	 Batch: 251 	 Loss: 1.638772964477539
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6466130018234253
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6049288511276245
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6316477060317993
Progress: 87.65% 	 Batch: 291 	 Loss: 1.643187403678894
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6476105451583862
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6468385457992554
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6370739936828613
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6194241046905518
Validation Loss: 2.166276146204044
Accuracy of the model (Test Data): 75.75%
Checkpoint ...
Epoch 53/250
Adjusted learning rate to 0.08933
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6692858934402466
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6559923887252808
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6415692567825317
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6508182287216187
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6644327640533447
Progress: 15.36% 	 Batch: 51 	 Loss: 1.624480128288269
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6488186120986938
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6670310497283936
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6185649633407593
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6458876132965088
Progress: 30.42% 	 Batch: 101 	 Loss: 1.656603455543518
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6606597900390625
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6519280672073364
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6399489641189575
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6520802974700928
Progress: 45.48% 	 Batch: 151 	 Loss: 1.665000081062317
Progress: 48.49% 	 Batch: 161 	 Loss: 1.665120005607605
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6454683542251587
Progress: 54.52% 	 Batch: 181 	 Loss: 1.63884437084198
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6279898881912231
Progress: 60.54% 	 Batch: 201 	 Loss: 1.644404411315918
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6390217542648315
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6372073888778687
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6292189359664917
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6417829990386963
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6360676288604736
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6608966588974
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6194664239883423
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6972837448120117
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6413882970809937
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6439546346664429
Progress: 93.67% 	 Batch: 311 	 Loss: 1.640121579170227
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6846609115600586
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6365586519241333
Validation Loss: 2.151082748021835
Accuracy of the model (Test Data): 75.87%
Checkpoint ...
Epoch 54/250
Adjusted learning rate to 0.08893
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6406753063201904
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6285086870193481
Progress: 6.33% 	 Batch: 21 	 Loss: 1.649053692817688
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6321568489074707
Progress: 12.35% 	 Batch: 41 	 Loss: 1.672216534614563
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6370103359222412
Progress: 18.37% 	 Batch: 61 	 Loss: 1.63433837890625
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6391828060150146
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6789239645004272
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6464563608169556
Progress: 30.42% 	 Batch: 101 	 Loss: 1.641635537147522
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6689666509628296
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6143014430999756
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6755797863006592
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6602421998977661
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6524041891098022
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6632078886032104
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6693390607833862
Progress: 54.52% 	 Batch: 181 	 Loss: 1.651680827140808
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6092129945755005
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6135170459747314
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6280579566955566
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6200954914093018
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6361353397369385
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6644502878189087
Progress: 75.60% 	 Batch: 251 	 Loss: 1.629676103591919
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6672135591506958
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6600110530853271
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6802257299423218
Progress: 87.65% 	 Batch: 291 	 Loss: 1.671890139579773
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6214373111724854
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6289455890655518
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6235524415969849
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6212471723556519
Validation Loss: 2.184941958158444
Accuracy of the model (Test Data): 74.96%
Checkpoint ...
Epoch 55/250
Adjusted learning rate to 0.08854
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.7015315294265747
Progress: 3.31% 	 Batch: 11 	 Loss: 1.649916172027588
Progress: 6.33% 	 Batch: 21 	 Loss: 1.688006043434143
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6661912202835083
Progress: 12.35% 	 Batch: 41 	 Loss: 1.61460542678833
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6352976560592651
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6306126117706299
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6433132886886597
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6306473016738892
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6370553970336914
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6786835193634033
Progress: 33.43% 	 Batch: 111 	 Loss: 1.653092384338379
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6351481676101685
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6369446516036987
Progress: 42.47% 	 Batch: 141 	 Loss: 1.647223949432373
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6406253576278687
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6380285024642944
Progress: 51.51% 	 Batch: 171 	 Loss: 1.639603853225708
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6552921533584595
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6544489860534668
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6245993375778198
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6533280611038208
Progress: 66.57% 	 Batch: 221 	 Loss: 1.627543330192566
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6148600578308105
Progress: 72.59% 	 Batch: 241 	 Loss: 1.650031328201294
Progress: 75.60% 	 Batch: 251 	 Loss: 1.644283652305603
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6403908729553223
Progress: 81.63% 	 Batch: 271 	 Loss: 1.654490351676941
Progress: 84.64% 	 Batch: 281 	 Loss: 1.666412591934204
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6496206521987915
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6658085584640503
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6523950099945068
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6377503871917725
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6557049751281738
Validation Loss: 2.1378848399871435
Accuracy of the model (Test Data): 74.55%
Checkpoint ...
Epoch 56/250
Adjusted learning rate to 0.08813
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6199548244476318
Progress: 3.31% 	 Batch: 11 	 Loss: 1.617103934288025
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6391886472702026
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6527658700942993
Progress: 12.35% 	 Batch: 41 	 Loss: 1.667999505996704
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6453579664230347
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6690378189086914
Progress: 21.39% 	 Batch: 71 	 Loss: 1.6606110334396362
Progress: 24.40% 	 Batch: 81 	 Loss: 1.569019079208374
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6455131769180298
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6728498935699463
Progress: 33.43% 	 Batch: 111 	 Loss: 1.636864423751831
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6171375513076782
Progress: 39.46% 	 Batch: 131 	 Loss: 1.64069402217865
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6431398391723633
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6536742448806763
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6388344764709473
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6769709587097168
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6775740385055542
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6511706113815308
Progress: 60.54% 	 Batch: 201 	 Loss: 1.664809226989746
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6672563552856445
Progress: 66.57% 	 Batch: 221 	 Loss: 1.641049861907959
Progress: 69.58% 	 Batch: 231 	 Loss: 1.637071967124939
Progress: 72.59% 	 Batch: 241 	 Loss: 1.644934892654419
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6368446350097656
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6492116451263428
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6428073644638062
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6439107656478882
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6470539569854736
Progress: 90.66% 	 Batch: 301 	 Loss: 1.654374599456787
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6416513919830322
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6266289949417114
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6156166791915894
Validation Loss: 2.215678322009551
Accuracy of the model (Test Data): 72.52%
Checkpoint ...
Epoch 57/250
Adjusted learning rate to 0.08772
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6481047868728638
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6955887079238892
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6433210372924805
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6606801748275757
Progress: 12.35% 	 Batch: 41 	 Loss: 1.630334496498108
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6600314378738403
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6383171081542969
Progress: 21.39% 	 Batch: 71 	 Loss: 1.633894681930542
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6511526107788086
Progress: 27.41% 	 Batch: 91 	 Loss: 1.636914849281311
Progress: 30.42% 	 Batch: 101 	 Loss: 1.648806095123291
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6176388263702393
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6474218368530273
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6619269847869873
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6492820978164673
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6365948915481567
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6397840976715088
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6349695920944214
Progress: 54.52% 	 Batch: 181 	 Loss: 1.627034068107605
Progress: 57.53% 	 Batch: 191 	 Loss: 1.646025538444519
Progress: 60.54% 	 Batch: 201 	 Loss: 1.622864842414856
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6435853242874146
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6331783533096313
Progress: 69.58% 	 Batch: 231 	 Loss: 1.6444060802459717
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6677254438400269
Progress: 75.60% 	 Batch: 251 	 Loss: 1.641154408454895
Progress: 78.61% 	 Batch: 261 	 Loss: 1.6422109603881836
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6663302183151245
Progress: 84.64% 	 Batch: 281 	 Loss: 1.643946647644043
Progress: 87.65% 	 Batch: 291 	 Loss: 1.6532503366470337
Progress: 90.66% 	 Batch: 301 	 Loss: 1.6321638822555542
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6534790992736816
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6353139877319336
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6042157411575317
Validation Loss: 2.144208950874133
Accuracy of the model (Test Data): 74.45%
Checkpoint ...
Epoch 58/250
Adjusted learning rate to 0.08731
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6289607286453247
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6643483638763428
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6392756700515747
Progress: 9.34% 	 Batch: 31 	 Loss: 1.662809133529663
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6187188625335693
Progress: 15.36% 	 Batch: 51 	 Loss: 1.662480354309082
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6774013042449951
Progress: 21.39% 	 Batch: 71 	 Loss: 1.601385235786438
Progress: 24.40% 	 Batch: 81 	 Loss: 1.62394380569458
Progress: 27.41% 	 Batch: 91 	 Loss: 1.627765417098999
Progress: 30.42% 	 Batch: 101 	 Loss: 1.6635416746139526
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6638246774673462
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6374797821044922
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6270240545272827
Progress: 42.47% 	 Batch: 141 	 Loss: 1.666845440864563
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6452305316925049
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6786831617355347
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6604050397872925
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6555241346359253
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6285444498062134
Progress: 60.54% 	 Batch: 201 	 Loss: 1.6428272724151611
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6571177244186401
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6222050189971924
Progress: 69.58% 	 Batch: 231 	 Loss: 1.603119134902954
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6337498426437378
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6307182312011719
Progress: 78.61% 	 Batch: 261 	 Loss: 1.649032473564148
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6322447061538696
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6238815784454346
Progress: 87.65% 	 Batch: 291 	 Loss: 1.658716082572937
Progress: 90.66% 	 Batch: 301 	 Loss: 1.663670301437378
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6655036211013794
Progress: 96.69% 	 Batch: 321 	 Loss: 1.6375514268875122
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6371536254882812
Validation Loss: 2.109589729553614
Accuracy of the model (Test Data): 76.90%
Saving the most accurate current model for the Test dataset
Checkpoint ...
Epoch 59/250
Adjusted learning rate to 0.08689
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6670194864273071
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6589391231536865
Progress: 6.33% 	 Batch: 21 	 Loss: 1.6765046119689941
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6474692821502686
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6475242376327515
Progress: 15.36% 	 Batch: 51 	 Loss: 1.641581654548645
Progress: 18.37% 	 Batch: 61 	 Loss: 1.612658143043518
Progress: 21.39% 	 Batch: 71 	 Loss: 1.613783597946167
Progress: 24.40% 	 Batch: 81 	 Loss: 1.6279970407485962
Progress: 27.41% 	 Batch: 91 	 Loss: 1.632040023803711
Progress: 30.42% 	 Batch: 101 	 Loss: 1.647080659866333
Progress: 33.43% 	 Batch: 111 	 Loss: 1.6493781805038452
Progress: 36.45% 	 Batch: 121 	 Loss: 1.6332858800888062
Progress: 39.46% 	 Batch: 131 	 Loss: 1.6343008279800415
Progress: 42.47% 	 Batch: 141 	 Loss: 1.6638078689575195
Progress: 45.48% 	 Batch: 151 	 Loss: 1.6437313556671143
Progress: 48.49% 	 Batch: 161 	 Loss: 1.6671905517578125
Progress: 51.51% 	 Batch: 171 	 Loss: 1.6352721452713013
Progress: 54.52% 	 Batch: 181 	 Loss: 1.6819318532943726
Progress: 57.53% 	 Batch: 191 	 Loss: 1.6501855850219727
Progress: 60.54% 	 Batch: 201 	 Loss: 1.640784502029419
Progress: 63.55% 	 Batch: 211 	 Loss: 1.6442080736160278
Progress: 66.57% 	 Batch: 221 	 Loss: 1.6241979598999023
Progress: 69.58% 	 Batch: 231 	 Loss: 1.646578311920166
Progress: 72.59% 	 Batch: 241 	 Loss: 1.6595968008041382
Progress: 75.60% 	 Batch: 251 	 Loss: 1.6446874141693115
Progress: 78.61% 	 Batch: 261 	 Loss: 1.673323154449463
Progress: 81.63% 	 Batch: 271 	 Loss: 1.6572595834732056
Progress: 84.64% 	 Batch: 281 	 Loss: 1.6645417213439941
Progress: 87.65% 	 Batch: 291 	 Loss: 1.645475149154663
Progress: 90.66% 	 Batch: 301 	 Loss: 1.644763708114624
Progress: 93.67% 	 Batch: 311 	 Loss: 1.6317845582962036
Progress: 96.69% 	 Batch: 321 	 Loss: 1.630537748336792
Progress: 99.70% 	 Batch: 331 	 Loss: 1.6386364698410034
Validation Loss: 2.1656781007082033
Accuracy of the model (Test Data): 74.95%
Checkpoint ...
Epoch 60/250
Adjusted learning rate to 0.08646
Train...
Progress: 0.30% 	 Batch: 1 	 Loss: 1.6532796621322632
Progress: 3.31% 	 Batch: 11 	 Loss: 1.6435842514038086
Progress: 6.33% 	 Batch: 21 	 Loss: 1.640155553817749
Progress: 9.34% 	 Batch: 31 	 Loss: 1.6464893817901611
Progress: 12.35% 	 Batch: 41 	 Loss: 1.6175404787063599
Progress: 15.36% 	 Batch: 51 	 Loss: 1.6962178945541382
Progress: 18.37% 	 Batch: 61 	 Loss: 1.6373364925384521
Progress: 21.39% 	 Batch: 71 	 Loss: 1.650037169456482
Progress: 24.40% 	 Batch: 81 	 Loss: 1.643439769744873
Progress: 27.41% 	 Batch: 91 	 Loss: 1.6304612159729004
